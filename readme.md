# Text Summarization Models Comparison Project
[![Python](https://img.shields.io/badge/Python-3.11.7-blue.svg)](https://www.python.org/)
[![Transformers](https://img.shields.io/badge/Transformers-latest-green.svg)](https://huggingface.co/transformers/)
[![ROUGE](https://img.shields.io/badge/ROUGE-latest-red.svg)](https://pypi.org/project/rouge/)

## ğŸ“Š Overview
Comprehensive comparison of three state-of# Text Summarization Models Comparison Project
[![Python](https://img.shields.io/badge/Python-3.11.7-blue.svg)](https://www.python.org/)
[![Transformers](https://img.shields.io/badge/Transformers-latest-green.svg)](https://huggingface.co/transformers/)
[![ROUGE](https://img.shields.io/badge/ROUGE-latest-red.svg)](https://pypi.org/project/rouge/)

## ğŸ“Š Overview
Comprehensive comparison of three state-of-the-art text summarization models: BERTSUM (extractive), T5 (abstractive), and BART (abstractive).

## ğŸ“ˆ Performance Metrics

| Model    | ROUGE-1 | ROUGE-2 | ROUGE-L |
|----------|---------|---------|----------|
| BERTSUM  | 0.96    | 0.92    | 0.96    |
| T5       | 0.57    | 0.49    | 0.53    |
| BART     | 0.55    | 0.47    | 0.55    |

## ğŸš€ Quick Start

```python
# Install dependencies
pip install transformers rouge torch




ğŸ—ï¸ Model Architectures
BERTSUM
Extractive summarization using BERT base model
Inter-sentence Transformer layers
Sentence importance classifier
T5
Text-to-text Transformer framework
Versatile encoder-decoder architecture
Pre-trained on diverse tasks
BART
Denoising autoencoder for text generation
Enhanced encoder-decoder model
Optimized for natural language generation
ğŸ”— Model Sources
BERT Base Uncased
T5 Small
BART Large CNN
ğŸ‘¥ Contributors
Youssef Ayman Mohamed (211000348)
ğŸ“š References
JMLR Paper
Hugging Face Documentation