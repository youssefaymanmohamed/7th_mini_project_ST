# Text Summarization Models Comparison Project
[![Python](https://img.shields.io/badge/Python-3.11.7-blue.svg)](https://www.python.org/)
[![Transformers](https://img.shields.io/badge/Transformers-latest-green.svg)](https://huggingface.co/transformers/)
[![ROUGE](https://img.shields.io/badge/ROUGE-latest-red.svg)](https://pypi.org/project/rouge/)

## 📊 Overview
Comprehensive comparison of three state-of# Text Summarization Models Comparison Project
[![Python](https://img.shields.io/badge/Python-3.11.7-blue.svg)](https://www.python.org/)
[![Transformers](https://img.shields.io/badge/Transformers-latest-green.svg)](https://huggingface.co/transformers/)
[![ROUGE](https://img.shields.io/badge/ROUGE-latest-red.svg)](https://pypi.org/project/rouge/)

## 📊 Overview
Comprehensive comparison of three state-of-the-art text summarization models: BERTSUM (extractive), T5 (abstractive), and BART (abstractive).

## 📈 Performance Metrics

| Model    | ROUGE-1 | ROUGE-2 | ROUGE-L |
|----------|---------|---------|----------|
| BERTSUM  | 0.96    | 0.92    | 0.96    |
| T5       | 0.57    | 0.49    | 0.53    |
| BART     | 0.55    | 0.47    | 0.55    |

## 🚀 Quick Start

```python
# Install dependencies
pip install transformers rouge torch




🏗️ Model Architectures
BERTSUM
Extractive summarization using BERT base model
Inter-sentence Transformer layers
Sentence importance classifier
T5
Text-to-text Transformer framework
Versatile encoder-decoder architecture
Pre-trained on diverse tasks
BART
Denoising autoencoder for text generation
Enhanced encoder-decoder model
Optimized for natural language generation
🔗 Model Sources
BERT Base Uncased
T5 Small
BART Large CNN
👥 Contributors
Youssef Ayman Mohamed (211000348)
📚 References
JMLR Paper
Hugging Face Documentation